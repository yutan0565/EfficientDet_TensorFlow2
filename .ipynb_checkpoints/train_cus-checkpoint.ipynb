{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5269fff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"efficient_det\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficient_net (EfficientNet) multiple                  6771296   \n",
      "_________________________________________________________________\n",
      "bi_fpn (BiFPN)               multiple                  129126    \n",
      "_________________________________________________________________\n",
      "box_class_predict (BoxClassP multiple                  249069    \n",
      "=================================================================\n",
      "Total params: 7,149,491\n",
      "Trainable params: 7,106,515\n",
      "Non-trainable params: 42,976\n",
      "_________________________________________________________________\n",
      "Epoch: 0/1 시작 \n",
      "step: 0/516.0, loss: 341.7989501953125\n",
      "step: 1/516.0, loss: 303.68975830078125\n",
      "step: 2/516.0, loss: 296.8872375488281\n",
      "step: 3/516.0, loss: 300.27239990234375\n",
      "step: 4/516.0, loss: 302.60858154296875\n",
      "step: 5/516.0, loss: 298.3941955566406\n",
      "step: 6/516.0, loss: 292.09063720703125\n",
      "step: 7/516.0, loss: 290.4656982421875\n",
      "step: 8/516.0, loss: 1178.946533203125\n",
      "step: 9/516.0, loss: 1221.7138671875\n",
      "step: 10/516.0, loss: 1473.504638671875\n",
      "step: 11/516.0, loss: 1372.6944580078125\n",
      "step: 12/516.0, loss: 1307.887939453125\n",
      "step: 13/516.0, loss: 1241.117431640625\n",
      "step: 14/516.0, loss: 1180.3438720703125\n",
      "step: 15/516.0, loss: 1122.4512939453125\n",
      "step: 16/516.0, loss: 1071.834716796875\n",
      "step: 17/516.0, loss: 1024.53955078125\n",
      "step: 18/516.0, loss: 984.6121215820312\n",
      "step: 19/516.0, loss: 946.8101806640625\n",
      "step: 20/516.0, loss: 913.9539794921875\n",
      "step: 21/516.0, loss: 882.276611328125\n",
      "step: 22/516.0, loss: 863.19287109375\n",
      "step: 23/516.0, loss: 838.7348022460938\n",
      "step: 24/516.0, loss: 1079.4324951171875\n",
      "step: 25/516.0, loss: 1055.73046875\n",
      "step: 26/516.0, loss: 1265.819580078125\n",
      "step: 27/516.0, loss: 1233.773193359375\n",
      "step: 28/516.0, loss: 1219.27783203125\n",
      "step: 29/516.0, loss: 1186.90869140625\n",
      "step: 30/516.0, loss: 1156.93896484375\n",
      "step: 31/516.0, loss: 1127.897705078125\n",
      "step: 32/516.0, loss: 1103.239990234375\n",
      "step: 33/516.0, loss: 1075.8951416015625\n",
      "step: 34/516.0, loss: 1051.0728759765625\n",
      "step: 35/516.0, loss: 1027.08154296875\n",
      "step: 36/516.0, loss: 1005.6919555664062\n",
      "step: 37/516.0, loss: 983.62451171875\n",
      "step: 38/516.0, loss: 962.5796508789062\n",
      "step: 39/516.0, loss: 943.5054931640625\n",
      "step: 40/516.0, loss: 928.0964965820312\n",
      "step: 41/516.0, loss: 910.11962890625\n",
      "step: 42/516.0, loss: 1023.5286865234375\n",
      "step: 43/516.0, loss: 1010.4064331054688\n",
      "step: 44/516.0, loss: 1113.94970703125\n",
      "step: 45/516.0, loss: 1120.194580078125\n",
      "step: 46/516.0, loss: 1103.3077392578125\n",
      "step: 47/516.0, loss: 1084.2957763671875\n",
      "step: 48/516.0, loss: 1069.4990234375\n",
      "step: 49/516.0, loss: 1052.735107421875\n",
      "step: 50/516.0, loss: 1036.4384765625\n",
      "step: 51/516.0, loss: 1117.6014404296875\n",
      "step: 52/516.0, loss: 1101.152099609375\n",
      "step: 53/516.0, loss: 1083.4525146484375\n",
      "step: 54/516.0, loss: 1110.1341552734375\n",
      "step: 55/516.0, loss: 1097.347412109375\n",
      "step: 56/516.0, loss: 1164.7354736328125\n",
      "step: 57/516.0, loss: 1165.60888671875\n",
      "step: 58/516.0, loss: 1150.921630859375\n",
      "step: 59/516.0, loss: 1134.788818359375\n",
      "step: 60/516.0, loss: 1119.6844482421875\n",
      "step: 61/516.0, loss: 1103.9132080078125\n",
      "step: 62/516.0, loss: 1088.343017578125\n",
      "step: 63/516.0, loss: 1073.7574462890625\n",
      "step: 64/516.0, loss: 1059.3619384765625\n",
      "step: 65/516.0, loss: 1047.6322021484375\n",
      "step: 66/516.0, loss: 1033.822998046875\n",
      "step: 67/516.0, loss: 1020.9291381835938\n",
      "step: 68/516.0, loss: 1010.7005004882812\n",
      "step: 69/516.0, loss: 998.136962890625\n",
      "step: 70/516.0, loss: 1045.1326904296875\n",
      "step: 71/516.0, loss: 1040.3809814453125\n",
      "step: 72/516.0, loss: 1040.6258544921875\n",
      "step: 73/516.0, loss: 1034.5074462890625\n",
      "step: 74/516.0, loss: 1022.6892700195312\n",
      "step: 75/516.0, loss: 1012.34521484375\n",
      "step: 76/516.0, loss: 1002.2046508789062\n",
      "step: 77/516.0, loss: 992.03564453125\n",
      "step: 78/516.0, loss: 983.271240234375\n",
      "step: 79/516.0, loss: 972.6826171875\n",
      "step: 80/516.0, loss: 962.2767333984375\n",
      "step: 81/516.0, loss: 952.08251953125\n",
      "step: 82/516.0, loss: 942.1021118164062\n",
      "step: 83/516.0, loss: 933.75830078125\n",
      "step: 84/516.0, loss: 924.2518310546875\n",
      "step: 85/516.0, loss: 915.0394897460938\n",
      "step: 86/516.0, loss: 905.921142578125\n",
      "step: 87/516.0, loss: 899.1048583984375\n",
      "step: 88/516.0, loss: 891.820068359375\n",
      "step: 89/516.0, loss: 890.0875244140625\n",
      "step: 90/516.0, loss: 881.8076171875\n",
      "step: 91/516.0, loss: 875.2404174804688\n",
      "step: 92/516.0, loss: 904.3172607421875\n",
      "step: 93/516.0, loss: 902.293701171875\n",
      "step: 94/516.0, loss: 893.9436645507812\n",
      "step: 95/516.0, loss: 886.798583984375\n",
      "step: 96/516.0, loss: 878.9703369140625\n",
      "step: 97/516.0, loss: 871.3590698242188\n",
      "step: 98/516.0, loss: 871.1185913085938\n",
      "step: 99/516.0, loss: 864.6068115234375\n",
      "step: 100/516.0, loss: 856.9716186523438\n",
      "step: 101/516.0, loss: 849.69384765625\n",
      "step: 102/516.0, loss: 842.8555908203125\n",
      "step: 103/516.0, loss: 835.5874633789062\n",
      "step: 104/516.0, loss: 829.5505981445312\n",
      "step: 105/516.0, loss: 832.0574951171875\n",
      "step: 106/516.0, loss: 825.1383056640625\n",
      "step: 107/516.0, loss: 846.600830078125\n",
      "step: 108/516.0, loss: 867.2882690429688\n",
      "step: 109/516.0, loss: 862.4730224609375\n",
      "step: 110/516.0, loss: 856.384765625\n",
      "step: 111/516.0, loss: 850.0235595703125\n",
      "step: 112/516.0, loss: 844.3434448242188\n",
      "step: 113/516.0, loss: 838.6663818359375\n",
      "step: 114/516.0, loss: 832.1014404296875\n",
      "step: 115/516.0, loss: 827.3587036132812\n",
      "step: 116/516.0, loss: 821.3176879882812\n",
      "step: 117/516.0, loss: 815.25048828125\n",
      "step: 118/516.0, loss: 809.0928344726562\n",
      "step: 119/516.0, loss: 803.1484985351562\n",
      "step: 120/516.0, loss: 797.2017822265625\n",
      "step: 121/516.0, loss: 796.2456665039062\n",
      "step: 122/516.0, loss: 811.0853881835938\n",
      "step: 123/516.0, loss: 805.3133544921875\n",
      "step: 124/516.0, loss: 819.0965576171875\n",
      "step: 125/516.0, loss: 813.1701049804688\n",
      "step: 126/516.0, loss: 807.417724609375\n",
      "step: 127/516.0, loss: 802.0953979492188\n",
      "step: 128/516.0, loss: 796.51513671875\n",
      "step: 129/516.0, loss: 791.0341796875\n",
      "step: 130/516.0, loss: 785.6617431640625\n",
      "step: 131/516.0, loss: 780.3151245117188\n",
      "step: 132/516.0, loss: 775.0097045898438\n",
      "step: 133/516.0, loss: 769.7555541992188\n",
      "step: 134/516.0, loss: 764.6107788085938\n",
      "step: 135/516.0, loss: 763.1643676757812\n",
      "step: 136/516.0, loss: 776.1891479492188\n",
      "step: 137/516.0, loss: 786.4843139648438\n",
      "step: 138/516.0, loss: 781.3643188476562\n",
      "step: 139/516.0, loss: 776.2745361328125\n",
      "step: 140/516.0, loss: 772.1215209960938\n",
      "step: 141/516.0, loss: 767.7985229492188\n",
      "step: 142/516.0, loss: 762.9078979492188\n",
      "step: 143/516.0, loss: 758.8129272460938\n",
      "step: 144/516.0, loss: 754.0595703125\n",
      "step: 145/516.0, loss: 749.26123046875\n",
      "step: 146/516.0, loss: 744.5731201171875\n",
      "step: 147/516.0, loss: 740.0173950195312\n",
      "step: 148/516.0, loss: 735.4737548828125\n",
      "step: 149/516.0, loss: 731.0064086914062\n",
      "step: 150/516.0, loss: 726.6097412109375\n",
      "step: 151/516.0, loss: 722.2042236328125\n",
      "step: 152/516.0, loss: 729.6268310546875\n",
      "step: 153/516.0, loss: 725.7255859375\n",
      "step: 154/516.0, loss: 732.8319091796875\n",
      "step: 155/516.0, loss: 730.4322509765625\n",
      "step: 156/516.0, loss: 726.12646484375\n",
      "step: 157/516.0, loss: 721.830322265625\n",
      "step: 158/516.0, loss: 717.8272705078125\n",
      "step: 159/516.0, loss: 713.899658203125\n",
      "step: 160/516.0, loss: 710.2877807617188\n",
      "step: 161/516.0, loss: 706.31787109375\n",
      "step: 162/516.0, loss: 702.3507080078125\n",
      "step: 163/516.0, loss: 698.3873291015625\n",
      "step: 164/516.0, loss: 694.4738159179688\n",
      "step: 165/516.0, loss: 690.6538696289062\n",
      "step: 166/516.0, loss: 687.2293090820312\n",
      "step: 167/516.0, loss: 683.470947265625\n",
      "step: 168/516.0, loss: 679.6980590820312\n",
      "step: 169/516.0, loss: 675.9951171875\n",
      "step: 170/516.0, loss: 681.1026611328125\n",
      "step: 171/516.0, loss: 686.0452270507812\n",
      "step: 172/516.0, loss: 691.3450927734375\n",
      "step: 173/516.0, loss: 695.8452758789062\n",
      "step: 174/516.0, loss: 692.2228393554688\n",
      "step: 175/516.0, loss: 688.5870971679688\n",
      "step: 176/516.0, loss: 685.4977416992188\n",
      "step: 177/516.0, loss: 682.3077392578125\n",
      "step: 178/516.0, loss: 678.7343139648438\n",
      "step: 179/516.0, loss: 675.47607421875\n",
      "step: 180/516.0, loss: 671.9317626953125\n",
      "step: 181/516.0, loss: 668.5170288085938\n",
      "step: 182/516.0, loss: 665.164794921875\n",
      "step: 183/516.0, loss: 661.78955078125\n",
      "step: 184/516.0, loss: 658.4152221679688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 185/516.0, loss: 655.3809204101562\n",
      "step: 186/516.0, loss: 652.1025390625\n",
      "step: 187/516.0, loss: 649.2916259765625\n",
      "step: 188/516.0, loss: 652.4683227539062\n",
      "step: 189/516.0, loss: 649.7174072265625\n",
      "step: 190/516.0, loss: 652.653076171875\n",
      "step: 191/516.0, loss: 649.7589111328125\n",
      "step: 192/516.0, loss: 647.5960693359375\n",
      "step: 193/516.0, loss: 644.471923828125\n",
      "step: 194/516.0, loss: 641.447265625\n",
      "step: 195/516.0, loss: 638.3955688476562\n",
      "step: 196/516.0, loss: 635.5515747070312\n",
      "step: 197/516.0, loss: 632.6294555664062\n",
      "step: 198/516.0, loss: 629.5953979492188\n",
      "step: 199/516.0, loss: 626.6348876953125\n",
      "step: 200/516.0, loss: 623.7827758789062\n",
      "step: 201/516.0, loss: 620.8771362304688\n",
      "step: 202/516.0, loss: 617.9744262695312\n",
      "step: 203/516.0, loss: 615.2418212890625\n",
      "step: 204/516.0, loss: 612.4269409179688\n",
      "step: 205/516.0, loss: 609.8871459960938\n",
      "step: 206/516.0, loss: 607.094482421875\n",
      "step: 207/516.0, loss: 604.4963989257812\n",
      "step: 208/516.0, loss: 601.8511352539062\n",
      "step: 209/516.0, loss: 599.1256103515625\n",
      "step: 210/516.0, loss: 596.5025634765625\n",
      "step: 211/516.0, loss: 593.8225708007812\n",
      "step: 212/516.0, loss: 591.1744995117188\n",
      "step: 213/516.0, loss: 593.4149169921875\n",
      "step: 214/516.0, loss: 590.802490234375\n",
      "step: 215/516.0, loss: 588.3863525390625\n",
      "step: 216/516.0, loss: 585.9061279296875\n",
      "step: 217/516.0, loss: 587.726806640625\n",
      "step: 218/516.0, loss: 585.4768676757812\n",
      "step: 219/516.0, loss: 587.1864013671875\n",
      "step: 220/516.0, loss: 588.8163452148438\n",
      "step: 221/516.0, loss: 586.3248901367188\n",
      "step: 222/516.0, loss: 583.9103393554688\n",
      "step: 223/516.0, loss: 581.4879150390625\n",
      "step: 224/516.0, loss: 579.036865234375\n",
      "step: 225/516.0, loss: 576.5928344726562\n",
      "step: 226/516.0, loss: 574.2509155273438\n",
      "step: 227/516.0, loss: 575.6126098632812\n",
      "step: 228/516.0, loss: 573.394775390625\n",
      "step: 229/516.0, loss: 571.1402587890625\n",
      "step: 230/516.0, loss: 568.7998046875\n",
      "step: 231/516.0, loss: 569.932861328125\n",
      "step: 232/516.0, loss: 567.8786010742188\n",
      "step: 233/516.0, loss: 568.9283447265625\n",
      "step: 234/516.0, loss: 566.644775390625\n",
      "step: 235/516.0, loss: 564.461669921875\n",
      "step: 236/516.0, loss: 562.322509765625\n",
      "step: 237/516.0, loss: 560.0686645507812\n",
      "step: 238/516.0, loss: 557.8587646484375\n",
      "step: 239/516.0, loss: 555.737060546875\n",
      "step: 240/516.0, loss: 553.5494384765625\n",
      "step: 241/516.0, loss: 551.8863525390625\n",
      "step: 242/516.0, loss: 549.8670043945312\n",
      "step: 243/516.0, loss: 547.770263671875\n",
      "step: 244/516.0, loss: 548.4937133789062\n",
      "step: 245/516.0, loss: 549.366455078125\n",
      "step: 246/516.0, loss: 550.0082397460938\n",
      "step: 247/516.0, loss: 550.5928955078125\n",
      "step: 248/516.0, loss: 548.5777587890625\n",
      "step: 249/516.0, loss: 546.479736328125\n",
      "step: 250/516.0, loss: 544.4747314453125\n",
      "step: 251/516.0, loss: 542.4365844726562\n",
      "step: 252/516.0, loss: 540.4368896484375\n",
      "step: 253/516.0, loss: 538.4974365234375\n",
      "step: 254/516.0, loss: 537.0638427734375\n",
      "step: 255/516.0, loss: 535.1436157226562\n",
      "step: 256/516.0, loss: 533.1343994140625\n",
      "step: 257/516.0, loss: 531.2554931640625\n",
      "step: 258/516.0, loss: 529.2849731445312\n",
      "step: 259/516.0, loss: 527.3558959960938\n",
      "step: 260/516.0, loss: 525.534423828125\n",
      "step: 261/516.0, loss: 523.6820068359375\n",
      "step: 262/516.0, loss: 521.8688354492188\n",
      "step: 263/516.0, loss: 520.4583740234375\n",
      "step: 264/516.0, loss: 520.6959228515625\n",
      "step: 265/516.0, loss: 520.9140625\n",
      "step: 266/516.0, loss: 519.5161743164062\n",
      "step: 267/516.0, loss: 517.654296875\n",
      "step: 268/516.0, loss: 515.8655395507812\n",
      "step: 269/516.0, loss: 514.0708618164062\n",
      "step: 270/516.0, loss: 512.2976684570312\n",
      "step: 271/516.0, loss: 510.58294677734375\n",
      "step: 272/516.0, loss: 508.9786071777344\n",
      "step: 273/516.0, loss: 507.466064453125\n",
      "step: 274/516.0, loss: 505.7730712890625\n",
      "step: 275/516.0, loss: 504.00213623046875\n",
      "step: 276/516.0, loss: 502.31939697265625\n",
      "step: 277/516.0, loss: 500.57806396484375\n",
      "step: 278/516.0, loss: 498.85260009765625\n",
      "step: 279/516.0, loss: 498.0115051269531\n",
      "step: 280/516.0, loss: 496.4314270019531\n",
      "step: 281/516.0, loss: 496.5155029296875\n",
      "step: 282/516.0, loss: 494.8190002441406\n",
      "step: 283/516.0, loss: 493.1393127441406\n",
      "step: 284/516.0, loss: 491.56787109375\n",
      "step: 285/516.0, loss: 489.939697265625\n",
      "step: 286/516.0, loss: 488.36541748046875\n",
      "step: 287/516.0, loss: 486.7467041015625\n",
      "step: 288/516.0, loss: 485.1231689453125\n",
      "step: 289/516.0, loss: 483.55523681640625\n",
      "step: 290/516.0, loss: 481.9631042480469\n",
      "step: 291/516.0, loss: 480.37286376953125\n",
      "step: 292/516.0, loss: 478.8571472167969\n",
      "step: 293/516.0, loss: 477.2852783203125\n",
      "step: 294/516.0, loss: 475.730712890625\n",
      "step: 295/516.0, loss: 474.2523193359375\n",
      "step: 296/516.0, loss: 472.72216796875\n",
      "step: 297/516.0, loss: 471.1920471191406\n",
      "step: 298/516.0, loss: 469.7203063964844\n",
      "step: 299/516.0, loss: 468.24639892578125\n",
      "step: 300/516.0, loss: 466.7500915527344\n",
      "step: 301/516.0, loss: 465.2570495605469\n",
      "step: 302/516.0, loss: 463.77392578125\n",
      "step: 303/516.0, loss: 462.31414794921875\n",
      "step: 304/516.0, loss: 460.86614990234375\n",
      "step: 305/516.0, loss: 459.41656494140625\n",
      "step: 306/516.0, loss: 457.9718017578125\n",
      "step: 307/516.0, loss: 456.5470886230469\n",
      "step: 308/516.0, loss: 455.6085205078125\n",
      "step: 309/516.0, loss: 454.2353210449219\n",
      "step: 310/516.0, loss: 452.8922424316406\n",
      "step: 311/516.0, loss: 451.80780029296875\n",
      "step: 312/516.0, loss: 450.4247131347656\n",
      "step: 313/516.0, loss: 449.0430908203125\n",
      "step: 314/516.0, loss: 447.674072265625\n",
      "step: 315/516.0, loss: 446.3293151855469\n",
      "step: 316/516.0, loss: 445.0142517089844\n",
      "step: 317/516.0, loss: 443.75830078125\n",
      "step: 318/516.0, loss: 442.4225158691406\n",
      "step: 319/516.0, loss: 441.0972595214844\n",
      "step: 320/516.0, loss: 439.7751770019531\n",
      "step: 321/516.0, loss: 438.476318359375\n",
      "step: 322/516.0, loss: 437.1815490722656\n",
      "step: 323/516.0, loss: 435.88446044921875\n",
      "step: 324/516.0, loss: 434.59100341796875\n",
      "step: 325/516.0, loss: 433.317138671875\n",
      "step: 326/516.0, loss: 433.352294921875\n",
      "step: 327/516.0, loss: 432.09466552734375\n",
      "step: 328/516.0, loss: 432.1145935058594\n",
      "step: 329/516.0, loss: 430.85198974609375\n",
      "step: 330/516.0, loss: 429.6052551269531\n",
      "step: 331/516.0, loss: 428.357177734375\n",
      "step: 332/516.0, loss: 427.1737976074219\n",
      "step: 333/516.0, loss: 425.978759765625\n",
      "step: 334/516.0, loss: 424.8015441894531\n",
      "step: 335/516.0, loss: 423.5910949707031\n",
      "step: 336/516.0, loss: 422.39056396484375\n",
      "step: 337/516.0, loss: 421.1881103515625\n",
      "step: 338/516.0, loss: 419.9850158691406\n",
      "step: 339/516.0, loss: 418.8478088378906\n",
      "step: 340/516.0, loss: 417.66839599609375\n",
      "step: 341/516.0, loss: 417.6581115722656\n",
      "step: 342/516.0, loss: 416.5257873535156\n",
      "step: 343/516.0, loss: 416.49908447265625\n",
      "step: 344/516.0, loss: 415.3336486816406\n",
      "step: 345/516.0, loss: 414.1797180175781\n",
      "step: 346/516.0, loss: 413.05078125\n",
      "step: 347/516.0, loss: 411.9119567871094\n",
      "step: 348/516.0, loss: 410.7774963378906\n",
      "step: 349/516.0, loss: 409.7159423828125\n",
      "step: 350/516.0, loss: 408.5833435058594\n",
      "step: 351/516.0, loss: 407.4764709472656\n",
      "step: 352/516.0, loss: 406.37408447265625\n",
      "step: 353/516.0, loss: 405.26214599609375\n",
      "step: 354/516.0, loss: 404.2058410644531\n",
      "step: 355/516.0, loss: 403.1060485839844\n",
      "step: 356/516.0, loss: 402.01751708984375\n",
      "step: 357/516.0, loss: 401.95343017578125\n",
      "step: 358/516.0, loss: 400.93402099609375\n",
      "step: 359/516.0, loss: 399.8579406738281\n",
      "step: 360/516.0, loss: 398.79046630859375\n",
      "step: 361/516.0, loss: 397.7507019042969\n",
      "step: 362/516.0, loss: 396.700927734375\n",
      "step: 363/516.0, loss: 395.6949768066406\n",
      "step: 364/516.0, loss: 394.6447448730469\n",
      "step: 365/516.0, loss: 393.61181640625\n",
      "step: 366/516.0, loss: 392.5811767578125\n",
      "step: 367/516.0, loss: 391.5426330566406\n",
      "step: 368/516.0, loss: 390.5312194824219\n",
      "step: 369/516.0, loss: 389.556884765625\n",
      "step: 370/516.0, loss: 388.53863525390625\n",
      "step: 371/516.0, loss: 387.5333557128906\n",
      "step: 372/516.0, loss: 386.5596923828125\n",
      "step: 373/516.0, loss: 386.4913635253906\n",
      "step: 374/516.0, loss: 386.41375732421875\n",
      "step: 375/516.0, loss: 385.47564697265625\n",
      "step: 376/516.0, loss: 384.489501953125\n",
      "step: 377/516.0, loss: 383.5442810058594\n",
      "step: 378/516.0, loss: 382.6012268066406\n",
      "step: 379/516.0, loss: 381.63116455078125\n",
      "step: 380/516.0, loss: 380.6953430175781\n",
      "step: 381/516.0, loss: 379.7361755371094\n",
      "step: 382/516.0, loss: 378.7818603515625\n",
      "step: 383/516.0, loss: 377.8265075683594\n",
      "step: 384/516.0, loss: 376.8856201171875\n",
      "step: 385/516.0, loss: 376.7973327636719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 386/516.0, loss: 375.8959655761719\n",
      "step: 387/516.0, loss: 374.98187255859375\n",
      "step: 388/516.0, loss: 374.9197998046875\n",
      "step: 389/516.0, loss: 373.9871826171875\n",
      "step: 390/516.0, loss: 373.8705749511719\n",
      "step: 391/516.0, loss: 372.9510498046875\n",
      "step: 392/516.0, loss: 372.0341796875\n",
      "step: 393/516.0, loss: 371.16595458984375\n",
      "step: 394/516.0, loss: 370.2532958984375\n",
      "step: 395/516.0, loss: 369.36273193359375\n",
      "step: 396/516.0, loss: 368.4664611816406\n",
      "step: 397/516.0, loss: 367.5733337402344\n",
      "step: 398/516.0, loss: 366.7052917480469\n",
      "step: 399/516.0, loss: 366.00665283203125\n",
      "step: 400/516.0, loss: 365.14141845703125\n",
      "step: 401/516.0, loss: 364.30914306640625\n",
      "step: 402/516.0, loss: 363.4541015625\n",
      "step: 403/516.0, loss: 362.59429931640625\n",
      "step: 404/516.0, loss: 362.4676818847656\n",
      "step: 405/516.0, loss: 361.6009521484375\n",
      "step: 406/516.0, loss: 360.76617431640625\n",
      "step: 407/516.0, loss: 359.91094970703125\n",
      "step: 408/516.0, loss: 359.0806579589844\n",
      "step: 409/516.0, loss: 358.92999267578125\n",
      "step: 410/516.0, loss: 358.0926513671875\n",
      "step: 411/516.0, loss: 357.2693176269531\n",
      "step: 412/516.0, loss: 356.4417419433594\n",
      "step: 413/516.0, loss: 355.61285400390625\n",
      "step: 414/516.0, loss: 354.8084716796875\n",
      "step: 415/516.0, loss: 354.08001708984375\n",
      "step: 416/516.0, loss: 353.2572021484375\n",
      "step: 417/516.0, loss: 352.4654846191406\n",
      "step: 418/516.0, loss: 351.67047119140625\n",
      "step: 419/516.0, loss: 350.8708190917969\n",
      "step: 420/516.0, loss: 350.06317138671875\n",
      "step: 421/516.0, loss: 349.2552185058594\n",
      "step: 422/516.0, loss: 349.1013488769531\n",
      "step: 423/516.0, loss: 348.3623962402344\n",
      "step: 424/516.0, loss: 347.7071838378906\n",
      "step: 425/516.0, loss: 346.9210205078125\n",
      "step: 426/516.0, loss: 346.1346130371094\n",
      "step: 427/516.0, loss: 345.3502502441406\n",
      "step: 428/516.0, loss: 345.18017578125\n",
      "step: 429/516.0, loss: 344.42303466796875\n",
      "step: 430/516.0, loss: 343.6475830078125\n",
      "step: 431/516.0, loss: 342.8697814941406\n",
      "step: 432/516.0, loss: 342.1131286621094\n",
      "step: 433/516.0, loss: 341.36517333984375\n",
      "step: 434/516.0, loss: 340.5989990234375\n",
      "step: 435/516.0, loss: 339.88787841796875\n",
      "step: 436/516.0, loss: 339.14007568359375\n",
      "step: 437/516.0, loss: 338.3880310058594\n",
      "step: 438/516.0, loss: 337.6420593261719\n",
      "step: 439/516.0, loss: 336.89654541015625\n",
      "step: 440/516.0, loss: 336.7250671386719\n",
      "step: 441/516.0, loss: 336.0178527832031\n",
      "step: 442/516.0, loss: 335.54791259765625\n",
      "step: 443/516.0, loss: 334.83648681640625\n",
      "step: 444/516.0, loss: 334.1679992675781\n",
      "step: 445/516.0, loss: 334.0374450683594\n",
      "step: 446/516.0, loss: 333.3715515136719\n",
      "step: 447/516.0, loss: 332.6476745605469\n",
      "step: 448/516.0, loss: 331.9271545410156\n",
      "step: 449/516.0, loss: 331.2166442871094\n",
      "step: 450/516.0, loss: 330.5140075683594\n",
      "step: 451/516.0, loss: 329.8085632324219\n",
      "step: 452/516.0, loss: 329.11505126953125\n",
      "step: 453/516.0, loss: 328.411376953125\n",
      "step: 454/516.0, loss: 327.7137756347656\n",
      "step: 455/516.0, loss: 327.0205993652344\n",
      "step: 456/516.0, loss: 326.3202209472656\n",
      "step: 457/516.0, loss: 325.6290588378906\n",
      "step: 458/516.0, loss: 325.4339599609375\n",
      "step: 459/516.0, loss: 324.8565368652344\n",
      "step: 460/516.0, loss: 324.17034912109375\n",
      "step: 461/516.0, loss: 323.49884033203125\n",
      "step: 462/516.0, loss: 322.8323059082031\n",
      "step: 463/516.0, loss: 322.1663513183594\n",
      "step: 464/516.0, loss: 321.5098876953125\n",
      "step: 465/516.0, loss: 320.8385314941406\n",
      "step: 466/516.0, loss: 320.1707458496094\n",
      "step: 467/516.0, loss: 319.5113220214844\n",
      "step: 468/516.0, loss: 318.8503112792969\n",
      "step: 469/516.0, loss: 318.1972961425781\n",
      "step: 470/516.0, loss: 317.5405578613281\n",
      "step: 471/516.0, loss: 316.8880310058594\n",
      "step: 472/516.0, loss: 316.2362060546875\n",
      "step: 473/516.0, loss: 315.6097106933594\n",
      "step: 474/516.0, loss: 314.96441650390625\n",
      "step: 475/516.0, loss: 314.3232727050781\n",
      "step: 476/516.0, loss: 314.1338195800781\n",
      "step: 477/516.0, loss: 313.49371337890625\n",
      "step: 478/516.0, loss: 312.8571472167969\n",
      "step: 479/516.0, loss: 312.2223205566406\n",
      "step: 480/516.0, loss: 311.6051940917969\n",
      "step: 481/516.0, loss: 310.9823303222656\n",
      "step: 482/516.0, loss: 310.3738708496094\n",
      "step: 483/516.0, loss: 309.74920654296875\n",
      "step: 484/516.0, loss: 309.1464538574219\n",
      "step: 485/516.0, loss: 308.5303039550781\n",
      "step: 486/516.0, loss: 307.9138488769531\n",
      "step: 487/516.0, loss: 307.3014221191406\n",
      "step: 488/516.0, loss: 306.6941223144531\n",
      "step: 489/516.0, loss: 306.084716796875\n",
      "step: 490/516.0, loss: 305.4847106933594\n",
      "step: 491/516.0, loss: 304.8831787109375\n",
      "step: 492/516.0, loss: 304.2973937988281\n",
      "step: 493/516.0, loss: 303.69720458984375\n",
      "step: 494/516.0, loss: 303.0998229980469\n",
      "step: 495/516.0, loss: 302.5087585449219\n",
      "step: 496/516.0, loss: 301.9390869140625\n",
      "step: 497/516.0, loss: 301.349609375\n",
      "step: 498/516.0, loss: 300.7662658691406\n",
      "step: 499/516.0, loss: 300.1845397949219\n",
      "step: 500/516.0, loss: 299.60296630859375\n",
      "step: 501/516.0, loss: 299.0372619628906\n",
      "step: 502/516.0, loss: 298.4582824707031\n",
      "step: 503/516.0, loss: 297.89910888671875\n",
      "step: 504/516.0, loss: 297.32501220703125\n",
      "step: 505/516.0, loss: 296.75921630859375\n",
      "step: 506/516.0, loss: 296.58990478515625\n",
      "step: 507/516.0, loss: 296.4216003417969\n",
      "step: 508/516.0, loss: 295.8741760253906\n",
      "step: 509/516.0, loss: 295.7427978515625\n",
      "step: 510/516.0, loss: 295.17755126953125\n",
      "step: 511/516.0, loss: 294.6189270019531\n",
      "step: 512/516.0, loss: 294.0591735839844\n",
      "step: 513/516.0, loss: 293.5163879394531\n",
      "step: 514/516.0, loss: 292.96282958984375\n",
      "step: 515/516.0, loss: 292.4263610839844\n",
      "step: 0/516.0, val_loss: 4.505491733551025\n",
      "step: 1/516.0, val_loss: 4.395790100097656\n",
      "step: 2/516.0, val_loss: 4.309563159942627\n",
      "step: 3/516.0, val_loss: 4.233642101287842\n",
      "step: 4/516.0, val_loss: 4.14617395401001\n",
      "step: 5/516.0, val_loss: 4.068460941314697\n",
      "step: 6/516.0, val_loss: 3.9590089321136475\n",
      "step: 7/516.0, val_loss: 3.862761974334717\n",
      "step: 8/516.0, val_loss: 8.948912620544434\n",
      "step: 9/516.0, val_loss: 8.980051040649414\n",
      "step: 10/516.0, val_loss: 9.713525772094727\n",
      "step: 11/516.0, val_loss: 9.10522174835205\n",
      "step: 12/516.0, val_loss: 8.647531509399414\n",
      "step: 13/516.0, val_loss: 8.215666770935059\n",
      "step: 14/516.0, val_loss: 7.857262134552002\n",
      "step: 15/516.0, val_loss: 7.5186357498168945\n",
      "step: 16/516.0, val_loss: 7.247509956359863\n",
      "step: 17/516.0, val_loss: 6.983182907104492\n",
      "step: 18/516.0, val_loss: 6.772677898406982\n",
      "step: 19/516.0, val_loss: 6.550589561462402\n",
      "step: 20/516.0, val_loss: 6.344637393951416\n",
      "step: 21/516.0, val_loss: 6.169132709503174\n",
      "step: 22/516.0, val_loss: 6.02463960647583\n",
      "step: 23/516.0, val_loss: 5.886524677276611\n",
      "step: 24/516.0, val_loss: 5.887682914733887\n",
      "step: 25/516.0, val_loss: 5.777878284454346\n",
      "step: 26/516.0, val_loss: 5.758986473083496\n",
      "step: 27/516.0, val_loss: 5.65297794342041\n",
      "step: 28/516.0, val_loss: 5.558498859405518\n",
      "step: 29/516.0, val_loss: 5.456176280975342\n",
      "step: 30/516.0, val_loss: 5.3625664710998535\n",
      "step: 31/516.0, val_loss: 5.26865291595459\n",
      "step: 32/516.0, val_loss: 5.191047191619873\n",
      "step: 33/516.0, val_loss: 5.1068925857543945\n",
      "step: 34/516.0, val_loss: 5.0379414558410645\n",
      "step: 35/516.0, val_loss: 4.979538917541504\n",
      "step: 36/516.0, val_loss: 4.920165538787842\n",
      "step: 37/516.0, val_loss: 4.853046417236328\n",
      "step: 38/516.0, val_loss: 4.794501781463623\n",
      "step: 39/516.0, val_loss: 4.7343831062316895\n",
      "step: 40/516.0, val_loss: 4.6861042976379395\n",
      "step: 41/516.0, val_loss: 4.632021427154541\n",
      "step: 42/516.0, val_loss: 4.599320888519287\n",
      "step: 43/516.0, val_loss: 4.555780410766602\n",
      "step: 44/516.0, val_loss: 4.523873329162598\n",
      "step: 45/516.0, val_loss: 4.4893412590026855\n",
      "step: 46/516.0, val_loss: 4.452589511871338\n",
      "step: 47/516.0, val_loss: 4.410384654998779\n",
      "step: 48/516.0, val_loss: 4.377078056335449\n",
      "step: 49/516.0, val_loss: 4.343653678894043\n",
      "step: 50/516.0, val_loss: 4.304785251617432\n",
      "step: 51/516.0, val_loss: 4.275289058685303\n",
      "step: 52/516.0, val_loss: 4.2435503005981445\n",
      "step: 53/516.0, val_loss: 4.209325790405273\n",
      "step: 54/516.0, val_loss: 4.210381507873535\n",
      "step: 55/516.0, val_loss: 4.187370777130127\n",
      "step: 56/516.0, val_loss: 4.158969879150391\n",
      "step: 57/516.0, val_loss: 4.1417155265808105\n",
      "step: 58/516.0, val_loss: 4.111812114715576\n",
      "step: 59/516.0, val_loss: 4.08287239074707\n",
      "step: 60/516.0, val_loss: 4.059968948364258\n",
      "step: 61/516.0, val_loss: 4.037900447845459\n",
      "step: 62/516.0, val_loss: 4.019514083862305\n",
      "step: 63/516.0, val_loss: 4.002104759216309\n",
      "step: 64/516.0, val_loss: 3.9861576557159424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 65/516.0, val_loss: 3.968839168548584\n",
      "step: 66/516.0, val_loss: 3.949174642562866\n",
      "step: 67/516.0, val_loss: 3.9254791736602783\n",
      "step: 68/516.0, val_loss: 3.9102797508239746\n",
      "step: 69/516.0, val_loss: 3.8886349201202393\n",
      "step: 70/516.0, val_loss: 3.865966320037842\n",
      "step: 71/516.0, val_loss: 3.8530588150024414\n",
      "step: 72/516.0, val_loss: 3.8413772583007812\n",
      "step: 73/516.0, val_loss: 3.8280556201934814\n",
      "step: 74/516.0, val_loss: 3.810959815979004\n",
      "step: 75/516.0, val_loss: 3.7906341552734375\n",
      "step: 76/516.0, val_loss: 3.7728569507598877\n",
      "step: 77/516.0, val_loss: 3.7538139820098877\n",
      "step: 78/516.0, val_loss: 3.7410888671875\n",
      "step: 79/516.0, val_loss: 3.726905107498169\n",
      "step: 80/516.0, val_loss: 3.712724208831787\n",
      "step: 81/516.0, val_loss: 3.7035703659057617\n",
      "step: 82/516.0, val_loss: 3.6944146156311035\n",
      "step: 83/516.0, val_loss: 3.685680627822876\n",
      "step: 84/516.0, val_loss: 3.6724350452423096\n",
      "step: 85/516.0, val_loss: 3.6568245887756348\n",
      "step: 86/516.0, val_loss: 3.641010046005249\n",
      "step: 87/516.0, val_loss: 3.6300928592681885\n",
      "step: 88/516.0, val_loss: 3.6196608543395996\n",
      "step: 89/516.0, val_loss: 3.609990119934082\n",
      "step: 90/516.0, val_loss: 3.6029858589172363\n",
      "step: 91/516.0, val_loss: 3.5917153358459473\n",
      "step: 92/516.0, val_loss: 3.5774447917938232\n",
      "step: 93/516.0, val_loss: 3.569472312927246\n",
      "step: 94/516.0, val_loss: 3.5573694705963135\n",
      "step: 95/516.0, val_loss: 3.544266939163208\n",
      "step: 96/516.0, val_loss: 3.5336291790008545\n",
      "step: 97/516.0, val_loss: 3.526966094970703\n",
      "step: 98/516.0, val_loss: 3.5202858448028564\n",
      "step: 99/516.0, val_loss: 3.5133328437805176\n",
      "step: 100/516.0, val_loss: 3.504542589187622\n",
      "step: 101/516.0, val_loss: 3.493114709854126\n",
      "step: 102/516.0, val_loss: 3.4808883666992188\n",
      "step: 103/516.0, val_loss: 3.4708974361419678\n",
      "step: 104/516.0, val_loss: 3.4627621173858643\n",
      "step: 105/516.0, val_loss: 3.462965250015259\n",
      "step: 106/516.0, val_loss: 3.455094337463379\n",
      "step: 107/516.0, val_loss: 3.4440805912017822\n",
      "step: 108/516.0, val_loss: 3.4331583976745605\n",
      "step: 109/516.0, val_loss: 3.4252824783325195\n",
      "step: 110/516.0, val_loss: 3.4159371852874756\n",
      "step: 111/516.0, val_loss: 3.40556263923645\n",
      "step: 112/516.0, val_loss: 3.395571708679199\n",
      "step: 113/516.0, val_loss: 3.3858985900878906\n",
      "step: 114/516.0, val_loss: 3.3804819583892822\n",
      "step: 115/516.0, val_loss: 3.3738622665405273\n",
      "step: 116/516.0, val_loss: 3.3705461025238037\n",
      "step: 117/516.0, val_loss: 3.366190195083618\n",
      "step: 118/516.0, val_loss: 3.3589367866516113\n",
      "step: 119/516.0, val_loss: 3.3513481616973877\n",
      "step: 120/516.0, val_loss: 3.3436667919158936\n",
      "step: 121/516.0, val_loss: 3.3392345905303955\n",
      "step: 122/516.0, val_loss: 3.3294169902801514\n",
      "step: 123/516.0, val_loss: 3.3245675563812256\n",
      "step: 124/516.0, val_loss: 3.3148348331451416\n",
      "step: 125/516.0, val_loss: 3.3108971118927\n",
      "step: 126/516.0, val_loss: 3.304366111755371\n",
      "step: 127/516.0, val_loss: 3.2982258796691895\n",
      "step: 128/516.0, val_loss: 3.2935707569122314\n",
      "step: 129/516.0, val_loss: 3.2896223068237305\n",
      "step: 130/516.0, val_loss: 3.2864532470703125\n",
      "step: 131/516.0, val_loss: 3.2799534797668457\n",
      "step: 132/516.0, val_loss: 3.2748239040374756\n",
      "step: 133/516.0, val_loss: 3.2697088718414307\n",
      "step: 134/516.0, val_loss: 3.2650606632232666\n",
      "step: 135/516.0, val_loss: 3.262266159057617\n",
      "step: 136/516.0, val_loss: 3.2532410621643066\n",
      "step: 137/516.0, val_loss: 3.2442288398742676\n",
      "step: 138/516.0, val_loss: 3.239830732345581\n",
      "step: 139/516.0, val_loss: 3.2353198528289795\n",
      "step: 140/516.0, val_loss: 3.2327721118927\n",
      "step: 141/516.0, val_loss: 3.2272205352783203\n",
      "step: 142/516.0, val_loss: 3.2223637104034424\n",
      "step: 143/516.0, val_loss: 3.219190835952759\n",
      "step: 144/516.0, val_loss: 3.2155160903930664\n",
      "step: 145/516.0, val_loss: 3.209700584411621\n",
      "step: 146/516.0, val_loss: 3.2065858840942383\n",
      "step: 147/516.0, val_loss: 3.200289011001587\n",
      "step: 148/516.0, val_loss: 3.1948671340942383\n",
      "step: 149/516.0, val_loss: 3.189847469329834\n",
      "step: 150/516.0, val_loss: 3.185112714767456\n",
      "step: 151/516.0, val_loss: 3.1808054447174072\n",
      "step: 152/516.0, val_loss: 3.1726019382476807\n",
      "step: 153/516.0, val_loss: 3.1679952144622803\n",
      "step: 154/516.0, val_loss: 3.1599128246307373\n",
      "step: 155/516.0, val_loss: 3.1574625968933105\n",
      "step: 156/516.0, val_loss: 3.154404401779175\n",
      "step: 157/516.0, val_loss: 3.1498687267303467\n",
      "step: 158/516.0, val_loss: 3.1459336280822754\n",
      "step: 159/516.0, val_loss: 3.141911268234253\n",
      "step: 160/516.0, val_loss: 3.1380014419555664\n",
      "step: 161/516.0, val_loss: 3.1329543590545654\n",
      "step: 162/516.0, val_loss: 3.1287569999694824\n",
      "step: 163/516.0, val_loss: 3.1246497631073\n",
      "step: 164/516.0, val_loss: 3.119621515274048\n",
      "step: 165/516.0, val_loss: 3.1151280403137207\n",
      "step: 166/516.0, val_loss: 3.11129093170166\n",
      "step: 167/516.0, val_loss: 3.1084885597229004\n",
      "step: 168/516.0, val_loss: 3.1044087409973145\n",
      "step: 169/516.0, val_loss: 3.1009087562561035\n",
      "step: 170/516.0, val_loss: 3.0938363075256348\n",
      "step: 171/516.0, val_loss: 3.0868165493011475\n",
      "step: 172/516.0, val_loss: 3.07979416847229\n",
      "step: 173/516.0, val_loss: 3.0726637840270996\n",
      "step: 174/516.0, val_loss: 3.06766939163208\n",
      "step: 175/516.0, val_loss: 3.0643181800842285\n",
      "step: 176/516.0, val_loss: 3.0610902309417725\n",
      "step: 177/516.0, val_loss: 3.0574254989624023\n",
      "step: 178/516.0, val_loss: 3.053954839706421\n",
      "step: 179/516.0, val_loss: 3.0519466400146484\n",
      "step: 180/516.0, val_loss: 3.0493853092193604\n",
      "step: 181/516.0, val_loss: 3.0480446815490723\n",
      "step: 182/516.0, val_loss: 3.0445683002471924\n",
      "step: 183/516.0, val_loss: 3.0405611991882324\n",
      "step: 184/516.0, val_loss: 3.0370283126831055\n",
      "step: 185/516.0, val_loss: 3.0322439670562744\n",
      "step: 186/516.0, val_loss: 3.0294482707977295\n",
      "step: 187/516.0, val_loss: 3.0267271995544434\n",
      "step: 188/516.0, val_loss: 3.0194413661956787\n",
      "step: 189/516.0, val_loss: 3.0167174339294434\n",
      "step: 190/516.0, val_loss: 3.0094897747039795\n",
      "step: 191/516.0, val_loss: 3.0071451663970947\n",
      "step: 192/516.0, val_loss: 3.004192352294922\n",
      "step: 193/516.0, val_loss: 3.000648021697998\n",
      "step: 194/516.0, val_loss: 2.998074531555176\n",
      "step: 195/516.0, val_loss: 2.9952690601348877\n",
      "step: 196/516.0, val_loss: 2.992501974105835\n",
      "step: 197/516.0, val_loss: 2.988957643508911\n",
      "step: 198/516.0, val_loss: 2.987117052078247\n",
      "step: 199/516.0, val_loss: 2.983518600463867\n",
      "step: 200/516.0, val_loss: 2.9806883335113525\n",
      "step: 201/516.0, val_loss: 2.977520227432251\n",
      "step: 202/516.0, val_loss: 2.9745306968688965\n",
      "step: 203/516.0, val_loss: 2.9716384410858154\n",
      "step: 204/516.0, val_loss: 2.9691412448883057\n",
      "step: 205/516.0, val_loss: 2.9674875736236572\n",
      "step: 206/516.0, val_loss: 2.9652562141418457\n",
      "step: 207/516.0, val_loss: 2.9625144004821777\n",
      "step: 208/516.0, val_loss: 2.9594335556030273\n",
      "step: 209/516.0, val_loss: 2.957399845123291\n",
      "step: 210/516.0, val_loss: 2.9565913677215576\n",
      "step: 211/516.0, val_loss: 2.9546563625335693\n",
      "step: 212/516.0, val_loss: 2.9525794982910156\n",
      "step: 213/516.0, val_loss: 2.946751117706299\n",
      "step: 214/516.0, val_loss: 2.9434943199157715\n",
      "step: 215/516.0, val_loss: 2.941434144973755\n",
      "step: 216/516.0, val_loss: 2.9405837059020996\n",
      "step: 217/516.0, val_loss: 2.9349708557128906\n",
      "step: 218/516.0, val_loss: 2.9328417778015137\n",
      "step: 219/516.0, val_loss: 2.9272522926330566\n",
      "step: 220/516.0, val_loss: 2.921632766723633\n",
      "step: 221/516.0, val_loss: 2.9189231395721436\n",
      "step: 222/516.0, val_loss: 2.91670298576355\n",
      "step: 223/516.0, val_loss: 2.913586139678955\n",
      "step: 224/516.0, val_loss: 2.910954236984253\n",
      "step: 225/516.0, val_loss: 2.9094290733337402\n",
      "step: 226/516.0, val_loss: 2.906270980834961\n",
      "step: 227/516.0, val_loss: 2.9004099369049072\n",
      "step: 228/516.0, val_loss: 2.898528575897217\n",
      "step: 229/516.0, val_loss: 2.896548271179199\n",
      "step: 230/516.0, val_loss: 2.895259380340576\n",
      "step: 231/516.0, val_loss: 2.889343023300171\n",
      "step: 232/516.0, val_loss: 2.8886659145355225\n",
      "step: 233/516.0, val_loss: 2.882692813873291\n",
      "step: 234/516.0, val_loss: 2.880197763442993\n",
      "step: 235/516.0, val_loss: 2.8774070739746094\n",
      "step: 236/516.0, val_loss: 2.874865770339966\n",
      "step: 237/516.0, val_loss: 2.872396469116211\n",
      "step: 238/516.0, val_loss: 2.8695218563079834\n",
      "step: 239/516.0, val_loss: 2.8689985275268555\n",
      "step: 240/516.0, val_loss: 2.8671374320983887\n",
      "step: 241/516.0, val_loss: 2.8657238483428955\n",
      "step: 242/516.0, val_loss: 2.865360975265503\n",
      "step: 243/516.0, val_loss: 2.863389492034912\n",
      "step: 244/516.0, val_loss: 2.8574132919311523\n",
      "step: 245/516.0, val_loss: 2.8514604568481445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 246/516.0, val_loss: 2.8454864025115967\n",
      "step: 247/516.0, val_loss: 2.8394687175750732\n",
      "step: 248/516.0, val_loss: 2.837501287460327\n",
      "step: 249/516.0, val_loss: 2.835888385772705\n",
      "step: 250/516.0, val_loss: 2.83329176902771\n",
      "step: 251/516.0, val_loss: 2.831716775894165\n",
      "step: 252/516.0, val_loss: 2.829288959503174\n",
      "step: 253/516.0, val_loss: 2.8274571895599365\n",
      "step: 254/516.0, val_loss: 2.8265175819396973\n",
      "step: 255/516.0, val_loss: 2.8263185024261475\n",
      "step: 256/516.0, val_loss: 2.8245980739593506\n",
      "step: 257/516.0, val_loss: 2.8255257606506348\n",
      "step: 258/516.0, val_loss: 2.824094295501709\n",
      "step: 259/516.0, val_loss: 2.8219497203826904\n",
      "step: 260/516.0, val_loss: 2.820620059967041\n",
      "step: 261/516.0, val_loss: 2.8193321228027344\n",
      "step: 262/516.0, val_loss: 2.8175852298736572\n",
      "step: 263/516.0, val_loss: 2.816150188446045\n",
      "step: 264/516.0, val_loss: 2.8102164268493652\n",
      "step: 265/516.0, val_loss: 2.804316997528076\n",
      "step: 266/516.0, val_loss: 2.8033196926116943\n",
      "step: 267/516.0, val_loss: 2.8014883995056152\n",
      "step: 268/516.0, val_loss: 2.799370765686035\n",
      "step: 269/516.0, val_loss: 2.797309398651123\n",
      "step: 270/516.0, val_loss: 2.795543670654297\n",
      "step: 271/516.0, val_loss: 2.793248176574707\n",
      "step: 272/516.0, val_loss: 2.7921431064605713\n",
      "step: 273/516.0, val_loss: 2.7907938957214355\n",
      "step: 274/516.0, val_loss: 2.790909767150879\n",
      "step: 275/516.0, val_loss: 2.7893142700195312\n",
      "step: 276/516.0, val_loss: 2.7887113094329834\n",
      "step: 277/516.0, val_loss: 2.7876741886138916\n",
      "step: 278/516.0, val_loss: 2.787263870239258\n",
      "step: 279/516.0, val_loss: 2.7865424156188965\n",
      "step: 280/516.0, val_loss: 2.7865467071533203\n",
      "step: 281/516.0, val_loss: 2.7811174392700195\n",
      "step: 282/516.0, val_loss: 2.7806100845336914\n",
      "step: 283/516.0, val_loss: 2.7790539264678955\n",
      "step: 284/516.0, val_loss: 2.7765493392944336\n",
      "step: 285/516.0, val_loss: 2.774806261062622\n",
      "step: 286/516.0, val_loss: 2.772305488586426\n",
      "step: 287/516.0, val_loss: 2.770418405532837\n",
      "step: 288/516.0, val_loss: 2.7700204849243164\n",
      "step: 289/516.0, val_loss: 2.769392490386963\n",
      "step: 290/516.0, val_loss: 2.7692081928253174\n",
      "step: 291/516.0, val_loss: 2.767756462097168\n",
      "step: 292/516.0, val_loss: 2.767500400543213\n",
      "step: 293/516.0, val_loss: 2.765949010848999\n",
      "step: 294/516.0, val_loss: 2.765260934829712\n",
      "step: 295/516.0, val_loss: 2.7651968002319336\n",
      "step: 296/516.0, val_loss: 2.764204740524292\n",
      "step: 297/516.0, val_loss: 2.7630388736724854\n",
      "step: 298/516.0, val_loss: 2.7619261741638184\n",
      "step: 299/516.0, val_loss: 2.75988507270813\n",
      "step: 300/516.0, val_loss: 2.7582859992980957\n",
      "step: 301/516.0, val_loss: 2.7568960189819336\n",
      "step: 302/516.0, val_loss: 2.75537109375\n",
      "step: 303/516.0, val_loss: 2.754094362258911\n",
      "step: 304/516.0, val_loss: 2.7528600692749023\n",
      "step: 305/516.0, val_loss: 2.751741647720337\n",
      "step: 306/516.0, val_loss: 2.7500391006469727\n",
      "step: 307/516.0, val_loss: 2.748959541320801\n",
      "step: 308/516.0, val_loss: 2.7479183673858643\n",
      "step: 309/516.0, val_loss: 2.74637508392334\n",
      "step: 310/516.0, val_loss: 2.74566912651062\n",
      "step: 311/516.0, val_loss: 2.7443346977233887\n",
      "step: 312/516.0, val_loss: 2.742933988571167\n",
      "step: 313/516.0, val_loss: 2.7409865856170654\n",
      "step: 314/516.0, val_loss: 2.739525318145752\n",
      "step: 315/516.0, val_loss: 2.7377593517303467\n",
      "step: 316/516.0, val_loss: 2.736023426055908\n",
      "step: 317/516.0, val_loss: 2.7344679832458496\n",
      "step: 318/516.0, val_loss: 2.732779026031494\n",
      "step: 319/516.0, val_loss: 2.732109785079956\n",
      "step: 320/516.0, val_loss: 2.7300970554351807\n",
      "step: 321/516.0, val_loss: 2.728261709213257\n",
      "step: 322/516.0, val_loss: 2.7265567779541016\n",
      "step: 323/516.0, val_loss: 2.724571943283081\n",
      "step: 324/516.0, val_loss: 2.723494052886963\n",
      "step: 325/516.0, val_loss: 2.7216575145721436\n",
      "step: 326/516.0, val_loss: 2.7192835807800293\n",
      "step: 327/516.0, val_loss: 2.7175111770629883\n",
      "step: 328/516.0, val_loss: 2.71515154838562\n",
      "step: 329/516.0, val_loss: 2.713951826095581\n",
      "step: 330/516.0, val_loss: 2.712162494659424\n",
      "step: 331/516.0, val_loss: 2.7113304138183594\n",
      "step: 332/516.0, val_loss: 2.710050106048584\n",
      "step: 333/516.0, val_loss: 2.708237886428833\n",
      "step: 334/516.0, val_loss: 2.706611394882202\n",
      "step: 335/516.0, val_loss: 2.7058029174804688\n",
      "step: 336/516.0, val_loss: 2.7046058177948\n",
      "step: 337/516.0, val_loss: 2.7033822536468506\n",
      "step: 338/516.0, val_loss: 2.7016243934631348\n",
      "step: 339/516.0, val_loss: 2.701356887817383\n",
      "step: 340/516.0, val_loss: 2.6999313831329346\n",
      "step: 341/516.0, val_loss: 2.6975455284118652\n",
      "step: 342/516.0, val_loss: 2.696315050125122\n",
      "step: 343/516.0, val_loss: 2.693875551223755\n",
      "step: 344/516.0, val_loss: 2.692199468612671\n",
      "step: 345/516.0, val_loss: 2.6908116340637207\n",
      "step: 346/516.0, val_loss: 2.689343214035034\n",
      "step: 347/516.0, val_loss: 2.6880345344543457\n",
      "step: 348/516.0, val_loss: 2.6865334510803223\n",
      "step: 349/516.0, val_loss: 2.685243606567383\n",
      "step: 350/516.0, val_loss: 2.684382438659668\n",
      "step: 351/516.0, val_loss: 2.6830742359161377\n",
      "step: 352/516.0, val_loss: 2.6822142601013184\n",
      "step: 353/516.0, val_loss: 2.6813642978668213\n",
      "step: 354/516.0, val_loss: 2.6793110370635986\n",
      "step: 355/516.0, val_loss: 2.6776015758514404\n",
      "step: 356/516.0, val_loss: 2.6765127182006836\n",
      "step: 357/516.0, val_loss: 2.6739697456359863\n",
      "step: 358/516.0, val_loss: 2.6727116107940674\n",
      "step: 359/516.0, val_loss: 2.6712677478790283\n",
      "step: 360/516.0, val_loss: 2.6704702377319336\n",
      "step: 361/516.0, val_loss: 2.6695408821105957\n",
      "step: 362/516.0, val_loss: 2.6681582927703857\n",
      "step: 363/516.0, val_loss: 2.666721820831299\n",
      "step: 364/516.0, val_loss: 2.665132761001587\n",
      "step: 365/516.0, val_loss: 2.6641712188720703\n",
      "step: 366/516.0, val_loss: 2.6625616550445557\n",
      "step: 367/516.0, val_loss: 2.6618075370788574\n",
      "step: 368/516.0, val_loss: 2.6605637073516846\n",
      "step: 369/516.0, val_loss: 2.659388303756714\n",
      "step: 370/516.0, val_loss: 2.6578714847564697\n",
      "step: 371/516.0, val_loss: 2.6559910774230957\n",
      "step: 372/516.0, val_loss: 2.655099391937256\n",
      "step: 373/516.0, val_loss: 2.653022050857544\n",
      "step: 374/516.0, val_loss: 2.650930166244507\n",
      "step: 375/516.0, val_loss: 2.649752616882324\n",
      "step: 376/516.0, val_loss: 2.6481947898864746\n",
      "step: 377/516.0, val_loss: 2.6464571952819824\n",
      "step: 378/516.0, val_loss: 2.6453933715820312\n",
      "step: 379/516.0, val_loss: 2.6441845893859863\n",
      "step: 380/516.0, val_loss: 2.6426477432250977\n",
      "step: 381/516.0, val_loss: 2.6419031620025635\n",
      "step: 382/516.0, val_loss: 2.640245199203491\n",
      "step: 383/516.0, val_loss: 2.6393086910247803\n",
      "step: 384/516.0, val_loss: 2.6380977630615234\n",
      "step: 385/516.0, val_loss: 2.6356961727142334\n",
      "step: 386/516.0, val_loss: 2.6350579261779785\n",
      "step: 387/516.0, val_loss: 2.6333250999450684\n",
      "step: 388/516.0, val_loss: 2.630828619003296\n",
      "step: 389/516.0, val_loss: 2.6292991638183594\n",
      "step: 390/516.0, val_loss: 2.626695156097412\n",
      "step: 391/516.0, val_loss: 2.6257829666137695\n",
      "step: 392/516.0, val_loss: 2.6246776580810547\n",
      "step: 393/516.0, val_loss: 2.6235451698303223\n",
      "step: 394/516.0, val_loss: 2.6229453086853027\n",
      "step: 395/516.0, val_loss: 2.6226930618286133\n",
      "step: 396/516.0, val_loss: 2.6216859817504883\n",
      "step: 397/516.0, val_loss: 2.6211130619049072\n",
      "step: 398/516.0, val_loss: 2.620243787765503\n",
      "step: 399/516.0, val_loss: 2.619457483291626\n",
      "step: 400/516.0, val_loss: 2.6186399459838867\n",
      "step: 401/516.0, val_loss: 2.6183338165283203\n",
      "step: 402/516.0, val_loss: 2.6168572902679443\n",
      "step: 403/516.0, val_loss: 2.6157498359680176\n",
      "step: 404/516.0, val_loss: 2.6128387451171875\n",
      "step: 405/516.0, val_loss: 2.6113829612731934\n",
      "step: 406/516.0, val_loss: 2.610123872756958\n",
      "step: 407/516.0, val_loss: 2.609678030014038\n",
      "step: 408/516.0, val_loss: 2.608652353286743\n",
      "step: 409/516.0, val_loss: 2.605773687362671\n",
      "step: 410/516.0, val_loss: 2.604987382888794\n",
      "step: 411/516.0, val_loss: 2.6034035682678223\n",
      "step: 412/516.0, val_loss: 2.6023049354553223\n",
      "step: 413/516.0, val_loss: 2.6013286113739014\n",
      "step: 414/516.0, val_loss: 2.600062370300293\n",
      "step: 415/516.0, val_loss: 2.599492073059082\n",
      "step: 416/516.0, val_loss: 2.5992047786712646\n",
      "step: 417/516.0, val_loss: 2.597841739654541\n",
      "step: 418/516.0, val_loss: 2.597109794616699\n",
      "step: 419/516.0, val_loss: 2.59599232673645\n",
      "step: 420/516.0, val_loss: 2.5949010848999023\n",
      "step: 421/516.0, val_loss: 2.594149589538574\n",
      "step: 422/516.0, val_loss: 2.5914511680603027\n",
      "step: 423/516.0, val_loss: 2.5908069610595703\n",
      "step: 424/516.0, val_loss: 2.5903091430664062\n",
      "step: 425/516.0, val_loss: 2.5894267559051514\n",
      "step: 426/516.0, val_loss: 2.5885908603668213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 427/516.0, val_loss: 2.5873749256134033\n",
      "step: 428/516.0, val_loss: 2.5847392082214355\n",
      "step: 429/516.0, val_loss: 2.5834991931915283\n",
      "step: 430/516.0, val_loss: 2.582383871078491\n",
      "step: 431/516.0, val_loss: 2.5816519260406494\n",
      "step: 432/516.0, val_loss: 2.580134153366089\n",
      "step: 433/516.0, val_loss: 2.579592704772949\n",
      "step: 434/516.0, val_loss: 2.5788869857788086\n",
      "step: 435/516.0, val_loss: 2.577853202819824\n",
      "step: 436/516.0, val_loss: 2.576633930206299\n",
      "step: 437/516.0, val_loss: 2.5757644176483154\n",
      "step: 438/516.0, val_loss: 2.575000047683716\n",
      "step: 439/516.0, val_loss: 2.5738372802734375\n",
      "step: 440/516.0, val_loss: 2.5714056491851807\n",
      "step: 441/516.0, val_loss: 2.570425271987915\n",
      "step: 442/516.0, val_loss: 2.569776773452759\n",
      "step: 443/516.0, val_loss: 2.568768262863159\n",
      "step: 444/516.0, val_loss: 2.5678741931915283\n",
      "step: 445/516.0, val_loss: 2.5654172897338867\n",
      "step: 446/516.0, val_loss: 2.564070224761963\n",
      "step: 447/516.0, val_loss: 2.5631935596466064\n",
      "step: 448/516.0, val_loss: 2.5619890689849854\n",
      "step: 449/516.0, val_loss: 2.5608394145965576\n",
      "step: 450/516.0, val_loss: 2.5598292350769043\n",
      "step: 451/516.0, val_loss: 2.558422327041626\n",
      "step: 452/516.0, val_loss: 2.5577657222747803\n",
      "step: 453/516.0, val_loss: 2.556710958480835\n",
      "step: 454/516.0, val_loss: 2.555692195892334\n",
      "step: 455/516.0, val_loss: 2.554610252380371\n",
      "step: 456/516.0, val_loss: 2.553938627243042\n",
      "step: 457/516.0, val_loss: 2.5526981353759766\n",
      "step: 458/516.0, val_loss: 2.550321578979492\n",
      "step: 459/516.0, val_loss: 2.5497360229492188\n",
      "step: 460/516.0, val_loss: 2.5489959716796875\n",
      "step: 461/516.0, val_loss: 2.548213481903076\n",
      "step: 462/516.0, val_loss: 2.547133684158325\n",
      "step: 463/516.0, val_loss: 2.546330451965332\n",
      "step: 464/516.0, val_loss: 2.5452346801757812\n",
      "step: 465/516.0, val_loss: 2.5443015098571777\n",
      "step: 466/516.0, val_loss: 2.5438175201416016\n",
      "step: 467/516.0, val_loss: 2.543505907058716\n",
      "step: 468/516.0, val_loss: 2.5422351360321045\n",
      "step: 469/516.0, val_loss: 2.541818857192993\n",
      "step: 470/516.0, val_loss: 2.541257619857788\n",
      "step: 471/516.0, val_loss: 2.540133237838745\n",
      "step: 472/516.0, val_loss: 2.5390079021453857\n",
      "step: 473/516.0, val_loss: 2.538121461868286\n",
      "step: 474/516.0, val_loss: 2.5375189781188965\n",
      "step: 475/516.0, val_loss: 2.5367960929870605\n",
      "step: 476/516.0, val_loss: 2.5352258682250977\n",
      "step: 477/516.0, val_loss: 2.534324884414673\n",
      "step: 478/516.0, val_loss: 2.5329418182373047\n",
      "step: 479/516.0, val_loss: 2.5318593978881836\n",
      "step: 480/516.0, val_loss: 2.531223773956299\n",
      "step: 481/516.0, val_loss: 2.530165910720825\n",
      "step: 482/516.0, val_loss: 2.5290920734405518\n",
      "step: 483/516.0, val_loss: 2.5281527042388916\n",
      "step: 484/516.0, val_loss: 2.5270586013793945\n",
      "step: 485/516.0, val_loss: 2.5265655517578125\n",
      "step: 486/516.0, val_loss: 2.5254480838775635\n",
      "step: 487/516.0, val_loss: 2.5249645709991455\n",
      "step: 488/516.0, val_loss: 2.524209976196289\n",
      "step: 489/516.0, val_loss: 2.5240886211395264\n",
      "step: 490/516.0, val_loss: 2.5229053497314453\n",
      "step: 491/516.0, val_loss: 2.5220675468444824\n",
      "step: 492/516.0, val_loss: 2.5216269493103027\n",
      "step: 493/516.0, val_loss: 2.5207362174987793\n",
      "step: 494/516.0, val_loss: 2.519610643386841\n",
      "step: 495/516.0, val_loss: 2.5186569690704346\n",
      "step: 496/516.0, val_loss: 2.5175533294677734\n",
      "step: 497/516.0, val_loss: 2.5163862705230713\n",
      "step: 498/516.0, val_loss: 2.515425443649292\n",
      "step: 499/516.0, val_loss: 2.5147626399993896\n",
      "step: 500/516.0, val_loss: 2.5138916969299316\n",
      "step: 501/516.0, val_loss: 2.5124731063842773\n",
      "step: 502/516.0, val_loss: 2.5111501216888428\n",
      "step: 503/516.0, val_loss: 2.5103342533111572\n",
      "step: 504/516.0, val_loss: 2.5094492435455322\n",
      "step: 505/516.0, val_loss: 2.508373975753784\n",
      "step: 506/516.0, val_loss: 2.5092687606811523\n",
      "step: 507/516.0, val_loss: 2.5097286701202393\n",
      "step: 508/516.0, val_loss: 2.5089173316955566\n",
      "step: 509/516.0, val_loss: 2.508054494857788\n",
      "step: 510/516.0, val_loss: 2.5073249340057373\n",
      "step: 511/516.0, val_loss: 2.506326675415039\n",
      "step: 512/516.0, val_loss: 2.5059096813201904\n",
      "step: 513/516.0, val_loss: 2.504927158355713\n",
      "step: 514/516.0, val_loss: 2.504142999649048\n",
      "step: 515/516.0, val_loss: 2.5028445720672607\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from core.efficientdet import EfficientDet, PostProcessing\n",
    "from data.dataloader import DetectionDataset, DataLoader\n",
    "from configuration import Config\n",
    "from utils.visualize import visualize_training_results\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def print_model_summary(network):\n",
    "    sample_inputs = tf.random.normal(shape=(Config.batch_size, Config.get_image_size()[0], Config.get_image_size()[1], Config.image_channels))\n",
    "    sample_outputs = network(sample_inputs, training=True)\n",
    "    network.summary()\n",
    "\n",
    "\n",
    "\n",
    "# GPU settings\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    # dataset\n",
    "    # train에 사용할 데이터셋을 불러오기\n",
    "train_dataset = DetectionDataset(\"train\")\n",
    "train_data, train_size = train_dataset.generate_datatset()\n",
    "data_loader = DataLoader()\n",
    "train_steps_per_epoch = tf.math.ceil(train_size / Config.batch_size)\n",
    "    # validation loss 계산에 사용할 데이터셋 불러오기\n",
    "valid_dataset = DetectionDataset(\"valid\")\n",
    "valid_data, valid_size = valid_dataset.generate_datatset()\n",
    "valid_steps_per_epoch = tf.math.ceil(train_size / Config.batch_size)\n",
    "\n",
    "    # model\n",
    "efficientdet = EfficientDet()\n",
    "print_model_summary(efficientdet)\n",
    "\n",
    "load_weights_from_epoch = Config.load_weights_from_epoch\n",
    "if Config.load_weights_before_training:\n",
    "    efficientdet.load_weights(filepath=Config.save_model_dir+\"epoch-{}\".format(load_weights_from_epoch))\n",
    "    print(\"Successfully load weights!\")\n",
    "else:\n",
    "    load_weights_from_epoch = -1\n",
    "\n",
    "post_process = PostProcessing()\n",
    "\n",
    "    # optimizer\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-4,\n",
    "                                                                 decay_steps=train_steps_per_epoch * Config.learning_rate_decay_epochs,\n",
    "                                                                 decay_rate=0.96)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "    # metrics\n",
    "loss_metric_train = tf.metrics.Mean()\n",
    "loss_metric_valid = tf.metrics.Mean()\n",
    "\n",
    "temp_loss_1 = []\n",
    "temp_loss_2 = []\n",
    "\n",
    "def train_step(batch_images, batch_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = efficientdet(batch_images, training=True)\n",
    "        loss_value = post_process.training_procedure(pred, batch_labels)\n",
    "    gradients = tape.gradient(target=loss_value, sources=efficientdet.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(gradients, efficientdet.trainable_variables))\n",
    "    loss_metric_train.update_state(values=loss_value)\n",
    "\n",
    "    # validation set에 대한 loss 계산해주는 함수\n",
    "def valid_step(batch_images, batch_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = efficientdet(batch_images, training=False)\n",
    "        loss_value = post_process.training_procedure(pred, batch_labels)\n",
    "    gradients = tape.gradient(target=loss_value, sources=efficientdet.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(gradients, efficientdet.trainable_variables))\n",
    "    loss_metric_valid.update_state(values=loss_value)\n",
    "\n",
    "    # early stop - loss 가 떨어지지 않는 경우 조정해주는 함수\n",
    "\n",
    "\n",
    "for epoch in range(load_weights_from_epoch + 1, Config.epochs):\n",
    "    print(\"Epoch: {}/{} 시작 \".format(epoch, Config.epochs))\n",
    "    for step, batch_data  in enumerate(train_data):\n",
    "        images_train, labels_train = data_loader.read_batch_data(batch_data)\n",
    "        train_step(images_train, labels_train)\n",
    "        print(\"step: {}/{}, loss: {}\".format(      step,\n",
    "                                                       train_steps_per_epoch,\n",
    "                                                       loss_metric_train.result()))\n",
    "        temp_loss_1.append(loss_metric_train.result())\n",
    "    loss_metric_train.reset_states()\n",
    "\n",
    "    for step, batch_data in enumerate(valid_data):\n",
    "        images, labels = data_loader.read_batch_data(batch_data)\n",
    "        valid_step(images, labels)\n",
    "        print(\"step: {}/{}, val_loss: {}\".format(step,\n",
    "                                                 valid_steps_per_epoch,\n",
    "                                                 loss_metric_valid.result()))\n",
    "        temp_loss_2.append(loss_metric_valid.result())\n",
    "    loss_metric_valid.reset_states()\n",
    "\n",
    "    if epoch % Config.save_frequency == 0:\n",
    "        efficientdet.save_weights(filepath=Config.save_model_dir+\"epoch-{}\".format(epoch), save_format=\"tf\")\n",
    "\n",
    "    if Config.test_images_during_training:\n",
    "        visualize_training_results(pictures=Config.test_images_dir_list, model=efficientdet, epoch=epoch)\n",
    "\n",
    "\n",
    "efficientdet.save_weights(filepath=Config.save_model_dir + \"saved_model\", save_format=\"tf\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03f92d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(temp_loss_1))\n",
    "\n",
    "plt.plot(x_len, temp_loss_1, marker='.', c='red', label=\"Train-set Loss\")\n",
    "plt.plot(x_len, temp_loss_2, marker='.', c='blue', label=\"Valid-set Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('step_loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
